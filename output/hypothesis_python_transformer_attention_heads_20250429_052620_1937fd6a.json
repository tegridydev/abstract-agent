{
  "topic": "python transformer attention heads",
  "outputs": {
    "Literature Citations": "Relevant papers from multiple sources:\n- [arXiv] GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability (2025)\n  http://arxiv.org/abs/2504.19414v1\n  Authors: Sehyeong Jo, Gangjae Jang, Haesol Park\n  Summary: The Vision Transformer (ViT) has made significant advancements in computer\nvision, utilizing self-attention mechanisms to achieve state-of-the-art\nperformance across various tasks, including image classification, object\ndetection, and segmentation. Its architectural flexibility and capabilities\nhave...\n- [arXiv] MoH: Multi-Head Attention as Mixture-of-Head Attention (2024)\n  http://arxiv.org/abs/2410.11842v1\n  Authors: Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan\n  Summary: In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention h...\n- [Semantic Scholar] RecurFormer: Not All Transformer Heads Need Self-Attention (2024)\n  https://www.semanticscholar.org/paper/642dbe8e74c94a61488414e3182b6e31e36f7f37\n  Authors: Ruiqing Yan, Linghan Zheng, Xingbo Du, Han Zou, Yufeng Guo, Jianfei Yang\n  Summary: Transformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the...\n- [arXiv] Attention-Only Transformers and Implementing MLPs with Attention Heads (2023)\n  http://arxiv.org/abs/2309.08593v1\n  Authors: Robert Huben, Valerie Morris\n  Summary: The transformer architecture is widely used in machine learning models and\nconsists of two alternating sublayers: attention heads and MLPs. We prove that\nan MLP neuron can be implemented by a masked attention head with internal\ndimension 1 so long as the MLP's activation function comes from a restri...\n- [Semantic Scholar] Improving Transformer-Based End-to-End Speaker Diarization by Assigning Auxiliary Losses to Attention Heads (2023)\n  https://www.semanticscholar.org/paper/d90e7f51c905dfbc201f1cfe978d83b51d16b2aa\n  Authors: Ye-Rin Jeoung, Joon-Young Yang, Jeonghwan Choi, Joon‐Hyuk Chang\n  Summary: Transformer-based end-to-end neural speaker diarization (EEND) models utilize the multi-head self-attention (SA) mechanism to enable accurate speaker label prediction in overlapped speech regions. In this study, to enhance the training effectiveness of SA-EEND models, we propose the use of auxiliary...\n- [Semantic Scholar] Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads (2023)\n  https://www.semanticscholar.org/paper/361d64b13896d8859b0eb138546ddd846da36b1b\n  Authors: Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, K. Tam\n  Summary: Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gend...\n- [Crossref] Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT (2023)\n  https://doi.org/10.18653/v1/2023.findings-emnlp.27\n  Authors: Soo Ryu\n  Summary: ...\n",
    "Citations List": [
      {
        "title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
        "authors": "Sehyeong Jo, Gangjae Jang, Haesol Park",
        "year": 2025,
        "summary": "The Vision Transformer (ViT) has made significant advancements in computer\nvision, utilizing self-attention mechanisms to achieve state-of-the-art\nperformance across various tasks, including image classification, object\ndetection, and segmentation. Its architectural flexibility and capabilities\nhave made it a preferred choice among researchers and practitioners. However,\nthe intricate multi-head attention mechanism of ViT presents significant\nchallenges to interpretability, as the underlying prediction process remains\nopaque. A critical limitation arises from an observation commonly noted in\ntransformer architectures: \"Not all attention heads are equally meaningful.\"\nOverlooking the relative importance of specific heads highlights the\nlimitations of existing interpretability methods. To address these challenges,\nwe introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel\nmethod that quantifies the importance of each attention head using\ngradient-based scores. These scores are normalized to derive a weighted\naggregate attention score, effectively capturing the relative contributions of\nindividual heads. GMAR clarifies the role of each head in the prediction\nprocess, enabling more precise interpretability at the head level. Experimental\nresults demonstrate that GMAR consistently outperforms traditional attention\nrollout techniques. This work provides a practical contribution to\ntransformer-based architectures, establishing a robust framework for enhancing\nthe interpretability of Vision Transformer models.",
        "url": "http://arxiv.org/abs/2504.19414v1",
        "source": "arXiv",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 1.0,
        "citation_score": 0.0,
        "composite_score": 0.6749990625023438
      },
      {
        "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
        "authors": "Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan",
        "year": 2024,
        "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
        "url": "http://arxiv.org/abs/2410.11842v1",
        "source": "arXiv",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 0.95,
        "citation_score": 0.0,
        "composite_score": 0.6599990625023437
      },
      {
        "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
        "authors": "Ruiqing Yan, Linghan Zheng, Xingbo Du, Han Zou, Yufeng Guo, Jianfei Yang",
        "year": 2024,
        "summary": "Transformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware, which focuses on local and short-range dependencies. Leveraging this insight, we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture. This replacement reduces the cache size without evicting tokens, thus maintaining generation quality. RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training. Experiments demonstrate that RecurFormer matches the original model's performance while significantly enhancing inference efficiency. Our approach provides a practical solution to the computational challenges of Transformer-based LLMs inference, making it highly attractive for tasks involving long inputs.",
        "url": "https://www.semanticscholar.org/paper/642dbe8e74c94a61488414e3182b6e31e36f7f37",
        "source": "Semantic Scholar",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 0.95,
        "citation_score": 0.0,
        "composite_score": 0.6599990625023437
      },
      {
        "title": "Attention-Only Transformers and Implementing MLPs with Attention Heads",
        "authors": "Robert Huben, Valerie Morris",
        "year": 2023,
        "summary": "The transformer architecture is widely used in machine learning models and\nconsists of two alternating sublayers: attention heads and MLPs. We prove that\nan MLP neuron can be implemented by a masked attention head with internal\ndimension 1 so long as the MLP's activation function comes from a restricted\nclass including SiLU and close approximations of ReLU and GeLU. This allows one\nto convert an MLP-and-attention transformer into an attention-only transformer\nat the cost of greatly increasing the number of attention heads. We also prove\nthat attention heads can perform the components of an MLP (linear\ntransformations and activation functions) separately. Finally, we prove that\nattention heads can encode arbitrary masking patterns in their weight matrices\nto within arbitrarily small error.",
        "url": "http://arxiv.org/abs/2309.08593v1",
        "source": "arXiv",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 0.9,
        "citation_score": 0.0,
        "composite_score": 0.6449990625023438
      },
      {
        "title": "Improving Transformer-Based End-to-End Speaker Diarization by Assigning Auxiliary Losses to Attention Heads",
        "authors": "Ye-Rin Jeoung, Joon-Young Yang, Jeonghwan Choi, Joon‐Hyuk Chang",
        "year": 2023,
        "summary": "Transformer-based end-to-end neural speaker diarization (EEND) models utilize the multi-head self-attention (SA) mechanism to enable accurate speaker label prediction in overlapped speech regions. In this study, to enhance the training effectiveness of SA-EEND models, we propose the use of auxiliary losses for the SA heads of the transformer layers. Specifically, we assume that the attention weight matrices of an SA layer are redundant if their patterns are similar to those of the identity matrix. We then explicitly constrain such matrices to exhibit specific speaker activity patterns relevant to voice activity detection or overlapped speech detection tasks. Consequently, we expect the proposed auxiliary losses to guide the transformer layers to exhibit more diverse patterns in the attention weights, thereby reducing the assumed redundancies in the SA heads. The effectiveness of the proposed method is demonstrated using the simulated and CALLHOME datasets for two-speaker diarization tasks, reducing the diarization error rate of the conventional SA-EEND model by 32.58% and 17.11%, respectively.",
        "url": "https://www.semanticscholar.org/paper/d90e7f51c905dfbc201f1cfe978d83b51d16b2aa",
        "source": "Semantic Scholar",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 0.9,
        "citation_score": 0.0,
        "composite_score": 0.6449990625023438
      },
      {
        "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
        "authors": "Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, K. Tam",
        "year": 2023,
        "summary": "Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.",
        "url": "https://www.semanticscholar.org/paper/361d64b13896d8859b0eb138546ddd846da36b1b",
        "source": "Semantic Scholar",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 0.9,
        "citation_score": 0.0,
        "composite_score": 0.6449990625023438
      },
      {
        "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
        "authors": "Soo Ryu",
        "year": 2023,
        "summary": "",
        "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.27",
        "source": "Crossref",
        "citations": 0,
        "relevance_score": 0.7499981250046875,
        "recency_score": 0.9,
        "citation_score": 0.0,
        "composite_score": 0.6449990625023438
      }
    ],
    "Literature Summary": "[Summarization failed: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download]",
    "Agent A": "[ERROR] Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
    "Agent B": "[ERROR] 'breakdown'",
    "Agent C": "[ERROR] 'breakdown'",
    "Agent D": "[ERROR] 'synthesis'",
    "Agent E": "[ERROR] 'novel_hypothesis'",
    "Novelty Assessment": "[Novelty detection failed: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download]"
  },
  "citations": [
    {
      "title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
      "authors": "Sehyeong Jo, Gangjae Jang, Haesol Park",
      "year": 2025,
      "summary": "The Vision Transformer (ViT) has made significant advancements in computer\nvision, utilizing self-attention mechanisms to achieve state-of-the-art\nperformance across various tasks, including image classification, object\ndetection, and segmentation. Its architectural flexibility and capabilities\nhave made it a preferred choice among researchers and practitioners. However,\nthe intricate multi-head attention mechanism of ViT presents significant\nchallenges to interpretability, as the underlying prediction process remains\nopaque. A critical limitation arises from an observation commonly noted in\ntransformer architectures: \"Not all attention heads are equally meaningful.\"\nOverlooking the relative importance of specific heads highlights the\nlimitations of existing interpretability methods. To address these challenges,\nwe introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel\nmethod that quantifies the importance of each attention head using\ngradient-based scores. These scores are normalized to derive a weighted\naggregate attention score, effectively capturing the relative contributions of\nindividual heads. GMAR clarifies the role of each head in the prediction\nprocess, enabling more precise interpretability at the head level. Experimental\nresults demonstrate that GMAR consistently outperforms traditional attention\nrollout techniques. This work provides a practical contribution to\ntransformer-based architectures, establishing a robust framework for enhancing\nthe interpretability of Vision Transformer models.",
      "url": "http://arxiv.org/abs/2504.19414v1",
      "source": "arXiv",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 1.0,
      "citation_score": 0.0,
      "composite_score": 0.6749990625023438
    },
    {
      "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
      "authors": "Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan",
      "year": 2024,
      "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
      "url": "http://arxiv.org/abs/2410.11842v1",
      "source": "arXiv",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 0.95,
      "citation_score": 0.0,
      "composite_score": 0.6599990625023437
    },
    {
      "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
      "authors": "Ruiqing Yan, Linghan Zheng, Xingbo Du, Han Zou, Yufeng Guo, Jianfei Yang",
      "year": 2024,
      "summary": "Transformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware, which focuses on local and short-range dependencies. Leveraging this insight, we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture. This replacement reduces the cache size without evicting tokens, thus maintaining generation quality. RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training. Experiments demonstrate that RecurFormer matches the original model's performance while significantly enhancing inference efficiency. Our approach provides a practical solution to the computational challenges of Transformer-based LLMs inference, making it highly attractive for tasks involving long inputs.",
      "url": "https://www.semanticscholar.org/paper/642dbe8e74c94a61488414e3182b6e31e36f7f37",
      "source": "Semantic Scholar",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 0.95,
      "citation_score": 0.0,
      "composite_score": 0.6599990625023437
    },
    {
      "title": "Attention-Only Transformers and Implementing MLPs with Attention Heads",
      "authors": "Robert Huben, Valerie Morris",
      "year": 2023,
      "summary": "The transformer architecture is widely used in machine learning models and\nconsists of two alternating sublayers: attention heads and MLPs. We prove that\nan MLP neuron can be implemented by a masked attention head with internal\ndimension 1 so long as the MLP's activation function comes from a restricted\nclass including SiLU and close approximations of ReLU and GeLU. This allows one\nto convert an MLP-and-attention transformer into an attention-only transformer\nat the cost of greatly increasing the number of attention heads. We also prove\nthat attention heads can perform the components of an MLP (linear\ntransformations and activation functions) separately. Finally, we prove that\nattention heads can encode arbitrary masking patterns in their weight matrices\nto within arbitrarily small error.",
      "url": "http://arxiv.org/abs/2309.08593v1",
      "source": "arXiv",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 0.9,
      "citation_score": 0.0,
      "composite_score": 0.6449990625023438
    },
    {
      "title": "Improving Transformer-Based End-to-End Speaker Diarization by Assigning Auxiliary Losses to Attention Heads",
      "authors": "Ye-Rin Jeoung, Joon-Young Yang, Jeonghwan Choi, Joon‐Hyuk Chang",
      "year": 2023,
      "summary": "Transformer-based end-to-end neural speaker diarization (EEND) models utilize the multi-head self-attention (SA) mechanism to enable accurate speaker label prediction in overlapped speech regions. In this study, to enhance the training effectiveness of SA-EEND models, we propose the use of auxiliary losses for the SA heads of the transformer layers. Specifically, we assume that the attention weight matrices of an SA layer are redundant if their patterns are similar to those of the identity matrix. We then explicitly constrain such matrices to exhibit specific speaker activity patterns relevant to voice activity detection or overlapped speech detection tasks. Consequently, we expect the proposed auxiliary losses to guide the transformer layers to exhibit more diverse patterns in the attention weights, thereby reducing the assumed redundancies in the SA heads. The effectiveness of the proposed method is demonstrated using the simulated and CALLHOME datasets for two-speaker diarization tasks, reducing the diarization error rate of the conventional SA-EEND model by 32.58% and 17.11%, respectively.",
      "url": "https://www.semanticscholar.org/paper/d90e7f51c905dfbc201f1cfe978d83b51d16b2aa",
      "source": "Semantic Scholar",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 0.9,
      "citation_score": 0.0,
      "composite_score": 0.6449990625023438
    },
    {
      "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
      "authors": "Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, K. Tam",
      "year": 2023,
      "summary": "Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.",
      "url": "https://www.semanticscholar.org/paper/361d64b13896d8859b0eb138546ddd846da36b1b",
      "source": "Semantic Scholar",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 0.9,
      "citation_score": 0.0,
      "composite_score": 0.6449990625023438
    },
    {
      "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
      "authors": "Soo Ryu",
      "year": 2023,
      "summary": "",
      "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.27",
      "source": "Crossref",
      "citations": 0,
      "relevance_score": 0.7499981250046875,
      "recency_score": 0.9,
      "citation_score": 0.0,
      "composite_score": 0.6449990625023438
    }
  ],
  "novelty_score": "[Novelty detection failed: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download]",
  "final_hypothesis": "[ERROR] 'novel_hypothesis'"
}